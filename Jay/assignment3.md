The focus of our senior design project is using AI agents to build data pipelines. From my academic perspective, this combines two important areas I’ve studied at the University of Cincinnati: artificial intelligence and data engineering. The goal is to automate processes that are often built manually today, such as data ingestion, transformation, and integration.

Through my time at UC, I’ve taken courses that directly prepare me for this project. In Database Theory (CS 4092), I learned about databases, SQL, and Postgres. In Natural Language Processing (CS 5134) and Deep Learning (CS 5173), I studied machine learning concepts and built up to understanding how Large Language Models (LLMs) work. In Software Engineering (), I learned the software development lifecycle and best practices for building projects in teams. Finally, in Artificial Intelligence (CS 4033), I explored the history of AI along with different algorithms and techniques. Together, these courses gave me the technical foundation for this project.

In my co-op at Tembo Data Systems, I was responsible for “all things data.” Working in a startup environment required me to wear many hats and learn quickly. I built and maintained end-to-end data pipelines, including ingestion, transformation, storage, and reporting through BI dashboards. I also fixed bugs in existing pipelines, which gave me first-hand experience with the challenges data engineers face. These experiences—both technical and non-technical—taught me adaptability, problem solving, and collaboration, which I plan to apply to this project. In my time there, I also worked on a end-to-end RAG (Retrieval-Augmented Generation) project that combined LLMs with a vector database to act as a Postgres Knowledge Expert. I was resoponsible to scraping the Postgres documentation, creating the vector database, and building the RAG pipeline to answer questions about Postgres. This experience gave me a first-hand look at how LLMs can be integrated with structured data, and the importance of data quality and relevance when working with AI systems.

I am excited about this project because it combines my interests in AI and data engineering. I enjoy working with data because it is what enables AI; without strong data pipelines, models cannot be trained effectively. Our plan is to start with a narrow use case, using only a handful of data sources and transformations, to build a proof of concept that we can then expand. With guidance from our advisor, Bo Brunton, Head of Product at Pantomath, we’ll gain insights into real-world challenges and shape our solution accordingly.

I hope to build a product that not only works but also has real-world use cases. Personally, I will measure success in two ways: what I contribute to the project and how much I learn along the way. My goal is to apply my background in data engineering while also expanding my knowledge of AI and AI agents. If I can deliver meaningful contributions and walk away with deeper expertise, I will consider this project a success. Also, in my opinion the project will be considered a success if the AI agent can build a data pipeline from end to end with limited human intervention.
