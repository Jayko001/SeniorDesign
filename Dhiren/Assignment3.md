My senior design project is an AI agent that can design and maintain end-to-end data pipelines—from source discovery and schema inference to code generation, testing, deployment, and continuous quality monitoring. I’m serving as the Backend & Design Lead, shaping the system architecture, developer experience, and reliability guardrails. I’m building this with Jay Kothari and Kaus Shankar under advisor Bo Brunton (Pantomath), and our weekly cadence keeps the team aligned while we iterate quickly on agent skills and evaluation loops. 
From my academic lens, this project targets the seam where software engineering meets data engineering and applied AI, with an emphasis on correctness, observability, and cost efficiency. The agent’s goal is opinionated: make high-quality pipelines the default by automating boilerplate and enforcing tests and data contracts out-of-the-box. Success for me looks like a system that is robust on messy, real-world data—not just clean demos.
My coursework directly scaffolds this build. EECE 3093C Software Engineering trained me to translate ambiguous requirements into sprints, interfaces, and tests; CS 4071 Design & Analysis of Algorithms informs planning for scalable transforms and search strategies inside the agent; CS 4092 Database Design/Development grounds schema design, normalization, and migration safety; and EECE 4029 Operating Systems helps with concurrency, resource limits, and container isolation. From CS 4065 Computer Networks, I draw on protocol-level thinking for reliable connectors and backoff policies; EECE 5132 Software Testing & QA sharpens coverage strategy (e.g., CACC) and property-based testing for data invariants; and CS 5167 User Interface guides the console where users inspect lineage, tests, and alerts. This term’s CS 5165 Intro to Cloud Computing, CS 5158 Data Security & Privacy, and CS 4033 AI Principles & Applications steer our cloud architecture, KMS-based secrets, governance, and LLM agent design; STAT 2037 Probability & Statistics I supports anomaly detection and SLOs; and ENGL 4092 Technical/Scientific Writing plus COMM 1071 Intro to Public Speech help me ship clear docs and demos. A Finance minor and ACCT 2081 Financial Accounting make me cost-literate, so I can justify architectural tradeoffs with real unit economics. 
My co-op and industry experience turn those classes into muscle memory. At Phillips Edison & Company (PECO) and Retransform (AI/ML Developer, Enterprise Applications), I built SQL stored procedures and Python ETL pipelines that moved critical business data with accuracy and traceability. I led Azure Cosmos DB container migrations, developed Power Automate flows, and used Excel/VBA for operational tooling—cutting repetitive manual work significantly and hardening processes against human error. I also worked on document abstraction pipelines (Azure AI Vision + LLMs) that trimmed turnaround time and unlocked structured data from PDFs at scale. Those tours taught me two non-negotiables: first, data quality and observability decide trust; second, change management beats raw cleverness. Practically, that means our agent will generate tests with every transform, emit lineage and metrics by default, and integrate seamlessly with existing tools so adoption is painless. The soft skills matter too: partner communication, requirement negotiation, and writing crisp runbooks—habits I’ll apply to our design docs, onboarding guides, and RCAs.
I’m fired up about this project because I’ve felt the friction of “just one more pipeline” in real companies and watched smart teams drown in glue code and manual QA. My preliminary approach is to build an agentic loop that: (1) profiles sources and infers contracts; (2) generates modular code (e.g., SQL/DBT/Python) with embedded tests; (3) provisions CI/CD and observability hooks; (4) runs an evaluation harness (synthetic + real fixtures) before promotion; and (5) monitors drift with automatic remediation suggestions. Guardrails include policy-as-code for PII handling, least-privilege secrets, and a change-budget that keeps cost and blast radius in check. We’ll stage capabilities—start with ingestion + transform scaffolding, then add quality checks, lineage, and finally auto-tuning (e.g., partitioning, file formats) driven by feedback loops. My role coordinates backend architecture and the operator UX so the agent is both reliable and a joy to use. This aligns with our team’s division of labor (Data Lead, AI Lead, Backend & Design Lead) and advisor cadence for feedback. 
Expected results: a working agent that can stand up a new, tested pipeline from a fresh data source in minutes; a web console that shows lineage, tests, SLAs, and costs; and integration points for Airflow/Prefect, dbt, and common warehouses. I’ll self-evaluate with hard metrics: time-to-first-pipeline, % pipelines with autogenerated tests, test pass rate, data quality SLI/SLOs (freshness, completeness, validity), runtime & cost per GB, MTTR on failures, and deployment lead time. Code quality gates (coverage thresholds, static analysis, reproducible builds), plus a usability score from peer users, will complement the numbers. I’ll know I did a good job when the team and advisor can trust the agent on unfamiliar data, our dashboards tell a truthful story without hand-holding, and new contributors can ship features quickly because the architecture is clear. Stretch outcomes include semantic lineage exploration and self-healing playbooks for common breakages—pushing us toward a zero-ETL future where the system is explainable, observable, and affordable by default.
